{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indeed.ca scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in ./common/.virtualenv/python3/lib/python3.5/site-packages\r\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import pandas\n",
    "from html.parser import HTMLParser\n",
    "from urllib.parse import urlparse\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link Construct Finish\n",
      "Analysing link: https://www.indeed.ca/jobs?q=data&l=Toronto&start=0\n",
      "Please wait...\n",
      "https://www.indeed.ca/jobs?q=data&l=Toronto&start=0\n",
      "Finish\n",
      "Analysing link: https://www.indeed.ca/jobs?q=data&l=Toronto&start=20\n",
      "Please wait...\n",
      "https://www.indeed.ca/jobs?q=data&l=Toronto&start=20\n",
      "Finish\n",
      "Analysing link: https://www.indeed.ca/jobs?q=data&l=Toronto&start=40\n",
      "Please wait...\n",
      "https://www.indeed.ca/jobs?q=data&l=Toronto&start=40\n",
      "Finish\n",
      "Analysing link: https://www.indeed.ca/jobs?q=data&l=Toronto&start=60\n",
      "Please wait...\n",
      "https://www.indeed.ca/jobs?q=data&l=Toronto&start=60\n",
      "Finish\n",
      "Analysing link: https://www.indeed.ca/jobs?q=data&l=Toronto&start=80\n",
      "Please wait...\n",
      "https://www.indeed.ca/jobs?q=data&l=Toronto&start=80\n",
      "Finish\n",
      "Already write job information into a csv file:dataInTorontoFromIndeed.csv\n",
      "You can use self.df for further processing.\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "class IndeedAnalyse(object):\n",
    "    \"\"\"Summary of the class here.\n",
    "\n",
    "    This class will get the job information based on keyword and location from\n",
    "    www.indeed.ca. You can access the attributes below.\n",
    "\n",
    "    Attributes:\n",
    "        keyword: Job query keywords and no default value.\n",
    "        location: Job query location and default value is 'toronto'.\n",
    "        pagenum: Number of pages you want to query. Every page has 20 jobs.\n",
    "            Default number is 10.\n",
    "        jobs: Jobs pool. Every job is a dictionary datatype stored in jobs\n",
    "            disctionary. The key of each job is 'id', and value contains:\n",
    "            job title, company name, job's link, short summary and long\n",
    "            summary.\n",
    "        df: If the args:flag in __init__ is Ture, all the job information will be \n",
    "            stored in a csv file. And then load the data into dataframe:df. \n",
    "            If flag is False, there is no df.\n",
    "    \"\"\"\n",
    "    def __init__(self, keyword, location='Toronto', pagenum=1, flag=True):\n",
    "        \"\"\"initialize query information and start query to get job information\n",
    "\n",
    "        First, initialize the query information like job keywords, job location\n",
    "        and the number of pages you want to query. Then use function:\n",
    "        link_construct() to initialize the links you want to query. At last,\n",
    "        traverse and analyse all the links to get the job information.\n",
    "\n",
    "        Args:\n",
    "            keyword: Job query keywords and no default value.\n",
    "            location: Job query location and default value is 'toronto'.\n",
    "            pagenum: Number of pages you want to query. Every page has 20 jobs.\n",
    "                Default number is 10.\n",
    "            flag: Whether write job information into a csv file and load data \n",
    "                into dataframe. Default value id True.\n",
    "        \"\"\"\n",
    "        self.keyword = keyword\n",
    "        self.location = location\n",
    "        self.pagenum = pagenum\n",
    "        self.flag = flag\n",
    "        self.jobs = {}\n",
    "        self.__indeed = \"https://www.indeed.ca\"\n",
    "        self.__links = []\n",
    "        self.link_construct()\n",
    "        for link in self.__links:\n",
    "            self.page_analyse(link)\n",
    "        self.write_into_csv(self.flag)\n",
    "        \n",
    "    def link_construct(self):\n",
    "        \"\"\"Construct indeed urls prepare to div_analyse\n",
    "\n",
    "        Urls in www.indeed.ca is regular and we can construct urls based on\n",
    "        rules. So after this function, we can get a list of links:__links to\n",
    "        analyse.\n",
    "        \"\"\"\n",
    "        for i in range(0, self.pagenum):\n",
    "            link = (\n",
    "                self.__indeed + '/jobs?q=' + self.keyword.replace(' ', '+')\n",
    "                + '&l=' + self.location.replace(' ', '+')\n",
    "                + '&start=' + str(i*20)\n",
    "            )\n",
    "            self.__links.append(link)\n",
    "        print('Link Construct Finish')\n",
    "        \n",
    "    def page_analyse(self, link):\n",
    "        \"\"\"Analyse single query result link\n",
    "\n",
    "        Analyse link from self.__link. First, find every job information by\n",
    "        tag class name or tag id. And then analyse tag block by div_analyse.\n",
    "\n",
    "        Args:\n",
    "            link: link from self.__link\n",
    "        \"\"\"\n",
    "        print('Analysing link: ' + link + '\\nPlease wait...')\n",
    "        print(link)\n",
    "        content = urlopen(link).read()\n",
    "        soup = BeautifulSoup(content, 'lxml')\n",
    "        #print(soup)\n",
    "        job_divs = soup.find(attrs={'id': 'resultsCol'}).find_all('div', {'class': re.compile(\"\\srow result\")})\n",
    "        for item in job_divs:\n",
    "\n",
    "            self.div_analyse(item)\n",
    "        print('Finish')\n",
    "\n",
    "    def div_analyse(self, jobdiv):\n",
    "        \"\"\"Analyse single job information in html tag block\n",
    "\n",
    "        Extract job information from html tag block. information include job\n",
    "        title, company name, url in indeed, id, short summary and long summary.\n",
    "        Long summary extract from url of job in indeed. Other information\n",
    "        extract from jobdiv.\n",
    "\n",
    "        Args:\n",
    "            jobdiv: html tag block contains single job infomation\n",
    "\n",
    "        Raises:\n",
    "            AttributeError: An error occured when extract long summary. If\n",
    "                there is no tag has id:job_summary in url of job, this error\n",
    "                will be catched and search html tag which class is\n",
    "                jobsearch-JobComponent-description.\n",
    "        \"\"\"\n",
    "        job = {}\n",
    "        key = jobdiv['id'][2:]\n",
    "        job['title'] = jobdiv.find(class_='jobtitle').a['title']\n",
    "        job['link'] = self.__indeed + jobdiv.find(class_='jobtitle').a['href']\n",
    "        job['company'] = jobdiv.find(class_ = 'company').get_text().strip(' \\t\\n\\r')\n",
    "        job['short_summary'] = jobdiv.find(class_ = 'summary').get_text().strip(' \\t\\n\\r')\n",
    "        job_content = urlopen(job['link']).read()\n",
    "        job_soup = BeautifulSoup(job_content, 'lxml')\n",
    "        try:\n",
    "            job['long_summary'] = job_soup.find(id='job_summary').get_text().strip('\\t\\n\\r').replace('\\n', ' ')\n",
    "        except AttributeError as error:\n",
    "            job['long_summary'] = job_soup.find(class_='jobsearch-JobComponent-description').get_text().strip('\\t\\n\\r').replace('\\n', ' ')\n",
    "            print('no span:job_summary switch to class:jobsearch-JobComponent-description')\n",
    "        if key in self.jobs:\n",
    "            if self.jobs[key]['link'] != job['link']:\n",
    "                print('WRONG: Same ID with different link')\n",
    "        else:\n",
    "            self.jobs[key] = job\n",
    "    \n",
    "    def write_into_csv(self, flag):\n",
    "        if flag:\n",
    "            #try:\n",
    "            filename = self.keyword + \"In\" + self.location + \"FromIndeed\"\n",
    "            with open(filename + \".csv\", \"w\", encoding=\"utf-8\") as toWrite:\n",
    "                writer = csv.writer(toWrite, delimiter=\",\")\n",
    "                writer.writerow([\"Id\", \"Job Title\", 'company', \"Short Summary\", \"Long Summary\", \"Link\"])\n",
    "                for key in self.jobs.keys():\n",
    "                    writer.writerow([key, self.jobs[key]['title'], self.jobs[key]['company'], self.jobs[key][\"short_summary\"], self.jobs[key][\"long_summary\"], self.jobs[key][\"link\"]])\n",
    "            print(\"Already write job information into a csv file:\" + filename + '.csv')\n",
    "            print(\"You can use self.df for further processing.\")\n",
    "            self.df = pandas.read_csv(filename+'.csv')\n",
    "            self.df\n",
    "        else:\n",
    "            return\n",
    "    \n",
    "s = IndeedAnalyse('data', 'Toronto', 5)\n",
    "print(len(s.jobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Job Title</th>\n",
       "      <th>company</th>\n",
       "      <th>Short Summary</th>\n",
       "      <th>Long Summary</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b5737fe55f7b3eaa</td>\n",
       "      <td>Junior Treasury Analyst</td>\n",
       "      <td>Liquid Capital Corp.</td>\n",
       "      <td>2-3 years of work experience in finance and pr...</td>\n",
       "      <td>Junior Treasury AnalystLiquid Capital Corp. – ...</td>\n",
       "      <td>https://www.indeed.ca/company/Liquid-Capital-C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ad62ddc40d502a7d</td>\n",
       "      <td>Data Analytics Analyst (Market Risk Technology)</td>\n",
       "      <td>Veritaaq</td>\n",
       "      <td>Given the current focus of both US and Canadia...</td>\n",
       "      <td>**Currently hiring for a top 4 financial insti...</td>\n",
       "      <td>https://www.indeed.ca/company/TBD/jobs/Data-An...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c0edbeca8721a306</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Health Quality Ontario</td>\n",
       "      <td>Assist with the design of data verification re...</td>\n",
       "      <td>Reporting to the Manager, QI Strategies and QI...</td>\n",
       "      <td>https://www.indeed.ca/rc/clk?jk=c0edbeca8721a3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aef737ef8f91785a</td>\n",
       "      <td>Digital Data Analyst (Digital and Data Science)</td>\n",
       "      <td>The Globe and Mail</td>\n",
       "      <td>Providing a centralized authority on how to pu...</td>\n",
       "      <td>Overview This role will be instrumental in hel...</td>\n",
       "      <td>https://www.indeed.ca/rc/clk?jk=aef737ef8f9178...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ea8e29eaef60858d</td>\n",
       "      <td>IT Manager, Data/Information Management</td>\n",
       "      <td>TD Bank</td>\n",
       "      <td>Experience in data management and data governa...</td>\n",
       "      <td>About This Role We are looking for someone to ...</td>\n",
       "      <td>https://www.indeed.ca/rc/clk?jk=ea8e29eaef6085...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Id                                        Job Title  \\\n",
       "0  b5737fe55f7b3eaa                          Junior Treasury Analyst   \n",
       "1  ad62ddc40d502a7d  Data Analytics Analyst (Market Risk Technology)   \n",
       "2  c0edbeca8721a306                                     Data Analyst   \n",
       "3  aef737ef8f91785a  Digital Data Analyst (Digital and Data Science)   \n",
       "4  ea8e29eaef60858d          IT Manager, Data/Information Management   \n",
       "\n",
       "                  company                                      Short Summary  \\\n",
       "0    Liquid Capital Corp.  2-3 years of work experience in finance and pr...   \n",
       "1                Veritaaq  Given the current focus of both US and Canadia...   \n",
       "2  Health Quality Ontario  Assist with the design of data verification re...   \n",
       "3      The Globe and Mail  Providing a centralized authority on how to pu...   \n",
       "4                 TD Bank  Experience in data management and data governa...   \n",
       "\n",
       "                                        Long Summary  \\\n",
       "0  Junior Treasury AnalystLiquid Capital Corp. – ...   \n",
       "1  **Currently hiring for a top 4 financial insti...   \n",
       "2  Reporting to the Manager, QI Strategies and QI...   \n",
       "3  Overview This role will be instrumental in hel...   \n",
       "4  About This Role We are looking for someone to ...   \n",
       "\n",
       "                                                Link  \n",
       "0  https://www.indeed.ca/company/Liquid-Capital-C...  \n",
       "1  https://www.indeed.ca/company/TBD/jobs/Data-An...  \n",
       "2  https://www.indeed.ca/rc/clk?jk=c0edbeca8721a3...  \n",
       "3  https://www.indeed.ca/rc/clk?jk=aef737ef8f9178...  \n",
       "4  https://www.indeed.ca/rc/clk?jk=ea8e29eaef6085...  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>experience</th>\n",
       "      <th>work</th>\n",
       "      <th>business</th>\n",
       "      <th>will</th>\n",
       "      <th>team</th>\n",
       "      <th>skills</th>\n",
       "      <th>working</th>\n",
       "      <th>knowledge</th>\n",
       "      <th>ability</th>\n",
       "      <th>...</th>\n",
       "      <th>engineering</th>\n",
       "      <th>duties</th>\n",
       "      <th>type</th>\n",
       "      <th>risk</th>\n",
       "      <th>driven</th>\n",
       "      <th>ll</th>\n",
       "      <th>operations</th>\n",
       "      <th>providing</th>\n",
       "      <th>1</th>\n",
       "      <th>statistical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>682</td>\n",
       "      <td>314</td>\n",
       "      <td>234</td>\n",
       "      <td>218</td>\n",
       "      <td>218</td>\n",
       "      <td>195</td>\n",
       "      <td>188</td>\n",
       "      <td>136</td>\n",
       "      <td>125</td>\n",
       "      <td>111</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   data  experience  work  business  will  team  skills  working  knowledge  \\\n",
       "0   682         314   234       218   218   195     188      136        125   \n",
       "\n",
       "   ability     ...       engineering  duties  type  risk  driven  ll  \\\n",
       "0      111     ...                38      38    37    37      37  37   \n",
       "\n",
       "   operations  providing   1  statistical  \n",
       "0          37         37  37           36  \n",
       "\n",
       "[1 rows x 100 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def data_cleaning(job_df):\n",
    "    \"\"\"\n",
    "    (DataFrame) -> list of strings\n",
    "    Takes in DataFrame of the job posting, produce a list of job summaries as list of strings\n",
    "    \"\"\"\n",
    "    summary_list = job_df['Long Summary'].tolist()\n",
    "    \n",
    "    with open('stop_words.txt',\"r\") as file2:\n",
    "        stop_words=file2.readlines()\n",
    "    stop_words = [e.strip(\"\\n\") for e in stop_words]\n",
    "    \n",
    "    file_list = summary_list\n",
    "    \n",
    "    for i in range(len(file_list)):\n",
    "        # remove html tags\n",
    "        cleanr = re.compile('<.*?>')\n",
    "        file_list[i] = re.sub(cleanr, '', file_list[i]) \n",
    "        \n",
    "        #replace html character codes\n",
    "        #parser = HTMLParser()\n",
    "        #html_decoded_string = parser.unescape(file_list[i])\n",
    "        #file_list[i] = html_decoded_string\n",
    "        \n",
    "        #remove urls\n",
    "        file_list[i] = re.sub(r'http\\S+', '', file_list[i])\n",
    "        \n",
    "        \n",
    "        #lowercase characters\n",
    "        file_list[i] = file_list[i].lower()\n",
    "        \n",
    "        #remove punctuation\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        file_list[i] = tokenizer.tokenize(file_list[i])\n",
    "        \n",
    "        #remove stopwords\n",
    "        cleaned_text = list(filter(lambda x: x not in stop_words, file_list[i]))\n",
    "        file_list[i] = cleaned_text\n",
    "        file_list[i] = \" \".join(file_list[i])\n",
    "    return file_list\n",
    "\n",
    "def find_top_word(data, n):\n",
    "    '''\n",
    "    (list of strings, int) -> DataFrame\n",
    "    \n",
    "    frequenct_df:\n",
    "    Takes in a list of tweets and number n, to select top n words with highest frequency, put \n",
    "    them into a DataFrame, where each row is each job summary, each column is each top n word's frequency\n",
    "    \n",
    "    frequency_summary_df:\n",
    "    On the basis of frequency_df, summarize all row values of each column\n",
    "    '''\n",
    "    counter = Counter()\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        counter.update([word for word in re.findall(r'\\w+', data[i])])\n",
    "      \n",
    "    # select most frequent words           \n",
    "    top_word = counter.most_common(n)  \n",
    "    top_word_list= []\n",
    "    \n",
    "    for j in range(len(data)):\n",
    "            temp_counter = Counter([word for word in re.findall(r'\\w+', data[j])]) \n",
    "            # if the word is in the tweet, count frequency, else frequency = 0\n",
    "            top_in_summary = [temp_counter[word] if temp_counter[word] > 0 else 0 for (word,wordCount) in top_word]\n",
    "            # create a list of top n words with highest frequencies\n",
    "            top_word_list.append(top_in_summary)\n",
    "               \n",
    "    frequency_df = pandas.DataFrame(top_word_list)\n",
    "    header = []\n",
    "    \n",
    "    for k in top_word:\n",
    "        header.append(k[0])\n",
    "    frequency_df.columns = header\n",
    "    \n",
    "    frequency_summary = dict(top_word)\n",
    "    frequency_summary_df = pandas.DataFrame(frequency_summary, index = [0])\n",
    "    frequency_summary_df = frequency_summary_df.T.sort_values(0, ascending=False).T\n",
    "    return frequency_df, frequency_summary_df\n",
    "\n",
    "top_word,top_word_summary = find_top_word(data_cleaning(s.df),100)\n",
    "top_word_summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
