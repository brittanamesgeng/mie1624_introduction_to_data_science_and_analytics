{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in ./common/.virtualenv/python3/lib/python3.5/site-packages\r\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MonsterAnalyse(object):\n",
    "    \"\"\"Summary of the class here.\n",
    "\n",
    "    This class will get the job information based on keyword and location from\n",
    "    www.indeed.ca. You can access the attributes below.\n",
    "\n",
    "    Attributes:\n",
    "        keyword: Job query keywords and no default value.\n",
    "        location: Job query location and default value is 'toronto'.\n",
    "        pagenum: Number of pages you want to query. Every page has 20 jobs.\n",
    "            Default number is 10.\n",
    "        jobs: Jobs pool. Every job is a dictionary datatype stored in jobs\n",
    "            disctionary. The key of each job is 'id', and value contains:\n",
    "            job title, company name, job's link, short summary and long\n",
    "            summary.\n",
    "        df: If the args:flag in __init__ is Ture, all the job information will be \n",
    "            stored in a csv file. And then load the data into dataframe:df. \n",
    "            If flag is False, there is no df.\n",
    "    \"\"\"\n",
    "    def __init__(self, keyword, location='Toronto', pagenum=10, flag=True):\n",
    "        \"\"\"initialize query information and start query to get job information\n",
    "\n",
    "        First, initialize the query information like job keywords, job location\n",
    "        and the number of pages you want to query. Then use function:\n",
    "        link_construct() to initialize the links you want to query. At last,\n",
    "        traverse and analyse all the links to get the job information.\n",
    "\n",
    "        Args:\n",
    "            keyword: Job query keywords and no default value.\n",
    "            location: Job query location and default value is 'toronto'.\n",
    "            pagenum: Number of pages you want to query. Every page has 20 jobs.\n",
    "                Default number is 10.\n",
    "            flag: Whether write job information into a csv file and load data \n",
    "                into dataframe. Default value id True.\n",
    "        \"\"\"\n",
    "        self.keyword = keyword\n",
    "        self.location = location\n",
    "        self.pagenum = pagenum\n",
    "        self.flag = flag\n",
    "        self.jobs = {}\n",
    "        self.__monster = \"https://www.monster.ca\"\n",
    "        self.__links = []\n",
    "        self.link_construct()\n",
    "        for link in self.__links:\n",
    "            self.page_analyse(link)\n",
    "        self.write_into_csv(self.flag)\n",
    "        \n",
    "    def link_construct(self):\n",
    "        \"\"\"Construct indeed urls prepare to div_analyse\n",
    "\n",
    "        Urls in www.indeed.ca is regular and we can construct urls based on\n",
    "        rules. So after this function, we can get a list of links:__links to\n",
    "        analyse.\n",
    "        \"\"\"\n",
    "        #https://www.monster.ca/jobs/search/?q=data&where=Toronto&jobid=193811123\n",
    "        \n",
    "        link = (\n",
    "            self.__monster + '/jobs/search/?q=' + self.keyword.replace(' ', '+')\n",
    "            + '&where=' + self.location.replace(' ', '+')\n",
    "            + '&page=' + str(self.pagenum)\n",
    "        )\n",
    "        self.__links.append(link)\n",
    "        print('Link Construct Finish')\n",
    "        \n",
    "    def page_analyse(self, link):\n",
    "        \"\"\"Analyse single query result link\n",
    "\n",
    "        Analyse link from self.__link. First, find every job information by\n",
    "        tag class name or tag id. And then analyse tag block by div_analyse.\n",
    "\n",
    "        Args:\n",
    "            link: link from self.__link\n",
    "        \"\"\"\n",
    "        print('Analysing link: ' + link + '\\nPlease wait...')\n",
    "        print(link)\n",
    "        content = urlopen(link).read()\n",
    "        soup = BeautifulSoup(content, 'lxml')\n",
    "\n",
    "        #job_divs = soup.find(attrs={'id': 'resultsCol'}).find_all('div', {'class': re.compile(\"\\srow result\")})\n",
    "        job_divs = soup.find_all('div',{'class':'flex-row'})\n",
    "        for item in job_divs:\n",
    "            self.div_analyse(item)\n",
    "        print('Finish')\n",
    "\n",
    "    def div_analyse(self, jobdiv):\n",
    "        \"\"\"Analyse single job information in html tag block\n",
    "\n",
    "        Extract job information from html tag block. information include job\n",
    "        title, company name, url in indeed, id, short summary and long summary.\n",
    "        Long summary extract from url of job in indeed. Other information\n",
    "        extract from jobdiv.\n",
    "\n",
    "        Args:\n",
    "            jobdiv: html tag block contains single job infomation\n",
    "\n",
    "        Raises:\n",
    "            AttributeError: An error occured when extract long summary. If\n",
    "                there is no tag has id:job_summary in url of job, this error\n",
    "                will be catched and search html tag which class is\n",
    "                jobsearch-JobComponent-description.\n",
    "        \"\"\"\n",
    "        job = {}\n",
    "        key = jobdiv.find(class_=\"title\").a['data-m_impr_j_jobid']\n",
    "        job['company'] = jobdiv.find(class_=\"company\").span.get_text()\n",
    "        job['title'] = jobdiv.find(class_=\"title\").span.get_text()\n",
    "        job['link'] = jobdiv.find(class_=\"title\").a[\"href\"]\n",
    "        \n",
    "        \n",
    "        job_content = urlopen(job['link']).read()\n",
    "        job_soup = BeautifulSoup(job_content, 'lxml')\n",
    "        try:\n",
    "            job['summary'] = job_soup.find(id='JobDescription').get_text().strip('\\t\\n\\r').replace('\\n', ' ')\n",
    "        except AttributeError as error:\n",
    "            job['summary'] = job_soup.find(class_='jobsearch-JobComponent-description').get_text().strip('\\t\\n\\r').replace('\\n', ' ')\n",
    "            print('no span:job_summary switch to class:jobsearch-JobComponent-description')\n",
    "        if key in self.jobs:\n",
    "            if self.jobs[key]['link'] != job['link']:\n",
    "                print('WRONG: Same ID with different link')\n",
    "        else:\n",
    "            self.jobs[key] = job\n",
    "    \n",
    "    def write_into_csv(self, flag):\n",
    "        if flag:\n",
    "            #try:\n",
    "            filename = self.keyword + \"In\" + self.location + \"FromMonster\"\n",
    "            with open(filename + \".csv\", \"w\", encoding=\"utf-8\") as toWrite:\n",
    "                writer = csv.writer(toWrite, delimiter=\",\")\n",
    "                writer.writerow([\"Id\", \"Job Title\", 'company', \"Summary\", \"Link\"])\n",
    "                for key in self.jobs.keys():\n",
    "                    writer.writerow([key, self.jobs[key]['title'], self.jobs[key]['company'], self.jobs[key][\"summary\"], self.jobs[key][\"link\"]])\n",
    "            print(\"Already write job information into a csv file:\" + filename + '.csv')\n",
    "            print(\"You can use self.df for further processing.\")\n",
    "            self.df = pandas.read_csv(filename+'.csv')\n",
    "            self.df\n",
    "        else:\n",
    "            return\n",
    "    \n",
    "s = IndeedAnalyse('data', 'Toronto', 2)\n",
    "print(len(s.jobs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
